
\documentclass[preprint,authoryear]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{proof}

\def\turn{\vdash}
\def\conv{\approx}

\newcommand{\fun}[1]{\textmd{#1}}
\newcommand{\cir}[1]{\textcircled{\raisebox{-0.9pt}{#1}}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{DRAFT}        % These are ignored unless
%% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Expressionless Weak-Head Normal Forms}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Larry Diehl}
           {Portland State University}
           {ldiehl@cs.pdx.edu}

\authorinfo{Tim Sheard}
           {Portland State University}
           {sheard@cs.pdx.edu}

\maketitle

\begin{abstract}
\lipsum[2-3]
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

The text of the paper begins here.

Lots of text.

More text.

Lots of text.

More text.


Lots of text.

More text.

Lots of text.

More text.

\section{Dependent Type Checking}

The motivation for weak-head normal forms in dependent type theory is
to strike a balance between simplicity and efficiency in the
implementation of the type checker. 

\subsection{Specification}

Before considering the {\it implementation} of a dependent type checker,
let's consider the complexities that arise in the
{\it specification} of dependent typing rules.
The rule for dependent application exposes the complexities
that arise in dependent type checking.

$$
\infer[]
  {\Gamma \turn f~a : B[a]}
{
  \Gamma \turn f : \Pi~A~B
  &
  \Gamma \turn a : A
}
$$

There are 3 interesting
constraints in this rule, and all of them have to do with what is
happening in the {\it type} position of the typing judgement.

\begin{enumerate}
\item The type of the function $f$ must be $\Pi~A~B$, thus it must at
  least be in head-normal form.
\item The type of $a$ must be equal to the domain of $f$.
\item The type of the application $f a$ is the substitution of $a$ for
  the bound variable in $B$.
\end{enumerate}

Because redexes can appear in dependent types, it is sometimes
necessary for types to be reduced before a typing rule becomes valid.
Otherwise, the following 3 problems can violate the aforementioned
constraints.

\begin{enumerate}
\item The type of $f$ is a redex like $(\lambda x. x)~(\Pi~A~B)$.
\item The type of $a$ is $A$, while the codomain of $f$ is a redex like
$(\lambda x. x)~A$.
\item Substituting the term $a$ into $B$ can create a redex, resulting
  in problems 1 and 2 somewhere else in type checking.
\end{enumerate}

The specification of typing rules can sweep all of these
problems under the rug by via a conversion rule.

$$
\infer[]
  {\Gamma \turn a : B}
{
  \Gamma \turn a : A
  &
  A \conv B
}
$$

The conversion relation $\conv$ considers types to equal after
reducing redexes. Hence $\conv$ is at least $\alpha$-$\beta$
equality, but it may enforce additional forms of equality, such as $\eta$.

\subsection{Implementation}

Dependent type theory can be specified succinctly because the
conversion relation is nondeterministic. In contrast, an
implementation of a dependent type checker in a functional language
must be deterministic. Additionally, the implementation must be
reasonably efficient to execute. 

Below we consider 4 different Haskell implementations of type checking
function application. The 4 implementations differ in the syntactic
grammar used for types: First expressions, then normal forms, then
weak-head normal forms with expression closures, and finally our novel
weak-head normal forms with expressionless closures.

\paragraph{Expression Types}

Consider the function case of a type checker for expressions below.
For simplicity, we assume that all terms are annotated enough for us
to infer types instead of checking them. Additionally, the terms are
in de Bruijn notation so we do not need to deal with $\alpha$-renaming.
The input of \texttt{infer} is an expression
value, and the output is an expression type in a type checking monad.

\begin{verbatim}
infer :: Exp -> TCM Exp
infer (App f a) = do
  Pi _A _B <- infer f
  _A'      <- infer a
  unless (_A == _A') $
    throwError "Domain not convertible to argument"
  return . norm (_B `sub` a)
\end{verbatim}

Whether or not the code above is correct depends on the rest of the
implementation of \texttt{infer}, what the conversion function
\texttt{(==)} does, and what the normalization function
\texttt{norm} does. Because the grammar of expressions is so flexible,
we can never be sure if a given expression is in normal form,
weak-head normal form, or if the head position is a redex.

The easiest way to get the implementation correct is to make
\texttt{norm} compute to full normal form. This way,
conversion \texttt{(==)} can be a simple syntactic equality. The
(albeit more complex) implementation makes \texttt{norm} compute only
to weak-head normal form, and conversion \texttt{(==)} first check
syntactic equality, and then compute away redexes if syntactic
equality fails. Whatever normal form we compute to, we need to make
sure that all cases of \texttt{infer} do it so that we can
successfully match against \texttt{Pi \_A \_B}.

Why is it a good idea to prefer weak-head normal forms over normal
forms? The normal forms can get quite large! Consider type checking
the application case where both the type of the argument (\texttt{\_A'})
and the domain (\texttt{\_A}) are the finite set type
\texttt{Fin (2 ** 1024)}. Computing both of these types to normal form
and before comparing them is not practical, while comparing the
terms with the redexes still present is reasonable.

\paragraph{Normal Form Types}

As mentioned above, computing to full normal form is not practical for
implementing a dependent type checker. Nevertheless, let us consider
the advantages obtained by having grammatically enforced normal form
types.

Below is an \texttt{infer} function that takes in an expression
and produces a type in normal form. The conversion function
\texttt{(==)} is just syntactic equality, and definition looks like
the expression version, except for the last line. In the expression
version we first syntactially substitute the argument \texttt{a} into
the codomain \texttt{B}, and then normalize the result. However,
syntactic substitution can create a redex, so it is not a well-formed
operation on normal forms. Instead, we first normalize the argument,
and then hereditarily substitute~\cite{TODO} into the codomain, which
computes away any redex created while substituting.

\begin{verbatim}
infer :: Exp -> TCM Nf
infer (App f a) = do
  Pi _A _B <- infer f
  _A'      <- infer a
  unless (_A == _A') $
    throwError "Domain not equal to argument"
  a' <- norm a
  return (_B `hsub` a')
\end{verbatim}

There are three major benefits we get by having grammatically enforced
normal forms types. The first two advantages have to do with
simplicity in the implementation, and allows us to have a type checker
with a minimal number of cases.

\begin{enumerate}
\item Because the type signature of \texttt{infer} enforces that it
  returns a normal form, our pattern match against \texttt{Pi \_A \_B}
  always works (we can't forget to compute to normal form in some case
  of \texttt{infer}).
\item The conversion function of \texttt{(==)} can be a simple
  equality check, which can even be derived by Haskell.
\item The grammar of expressions and normal forms are defined
  independently, which can be exploited to implement a very small type
  checker!
\end{enumerate}

The last benefit of normal form types, which we call the
 {\it independence} property, needs more explaining. Think of
the expressions as a minimal surface language, and the
normal forms as a core language. The only expression constructors that we
need are variables, functions, and applications. Normal forms have the
same constructors, but also constructors for the types, introduction,
and elimination rules of all non-function types in the languages (e.g.
booleans, natural numbers, lists, trees, products, sums, etc).

\begin{verbatim}
data Exp = EVar Int | ELam (Bind Exp) | EApp Exp Exp

data Nf = Type | Nat | Pi Nf (Bind Nf)
  | Zero | Suc Nf | Lam (Bind Nf) | Ne Ne

data Ne = Var Int | App Ne Nf
  | ElimNat (Bind Nf) Nf (Bind2 Nf) Ne
\end{verbatim}

How is it possible to have such a small expression grammar? Type
formers, constructors, and eliminators for all other types can be
exposed as definitions in an initial environment. If a normal form
primitive has multiple arguments, it can be wrapped with lambdas. This
means that a primitive like \texttt{Suc} is exposed as a variable
whose type is \texttt{Nat => Nat}, so users can even partially apply
them instead of needing to use the fully-applied primitives. 

\begin{verbatim}
zeroT = Nat
zero = Zero

sucT = Nat :=> Nat
suc = Lam (Bind (Suc (Var 0)))

elimNatT = Pi (Nat :=> Type)
  (Bind (Nat
     :=> Pi Nat (Bind (App (Var 1) (Var 0)
            :=> App (Var 1) (Suc (Var 0))))
     :=> Pi Nat (Bind (App (Var 1) (Var 0)))
  ))
elimNat = Lam (Bind (Lam (Bind (Lam (Bind
  ElimNat
    (Bind (Var 1 `App` Var 0))
    (Var 1)
    (Bind2 (Var 4 `App` Var 0 `App` Var 1))
    (Var 3)
  )))))

initCtx = [zeroT, sucT, elimNatT]
initEnv = [zero, suc, elimNat]
\end{verbatim}

The initial context \texttt{initCtx} contains types for all
non-$\Pi$ primitive constructions, and is used by \texttt{infer} in the
variable case.
The initial environment \texttt{initEnv} contains values for all
non-$\Pi$ primitive constructions, and is used by \texttt{norm} in the
variable case.

So what have we achieved by using this technique of embedding primitives
in an initial context and environment? Now our type checker
(\texttt{infer}) and our normalizer (\texttt{norm}) only need to
handle 3 cases, one for each constructor in the expression grammar!
This is quite nice, as a dependent type checker gets complicated
quickly as you add new feature. For example, if we added
Matita-style~\cite{TODO} elaboration of implicit arguments to
metavariables and unification problems, we would only need to
implement it for 3 cases of our \texttt{Exp} grammar.
Furthermore, changes to our primitives no longer require changes to
the type checker.

\paragraph{WHNF Types with Expressions}

So far we have seen a dependent type checker that uses
expression types, which may be done efficiently or inefficiently but
has no grammatical correctness properties, and a checker that uses
normal form types, which is inefficient but has grammatically enforced
correctness properties.

Recall that in the land of expression types we wanted to
make type checking reasonably efficient by
comparing types for syntactic equality before reducing a redex.
A popular way to do this {\it with} grammatically enforced correctness
properties is to implement an environment machine evaluator. Therein
an expression evalutes to a WHNF value, where constructor heads
(like \texttt{Suc}) store an environment along with an unevaluated
expression. The environment only contains WHNF's for all free
variables in the expression, but not for binding positions (like the
\texttt{Lam} body or the \texttt{Pi} codomain).

We are free to choose which terms we would like to contain closures.
For example, maybe we only want closures at binding positions. 

\begin{verbatim}
data Exp = EType | ENat | EPi Exp (Bind Exp)
  | EZero | ESuc Exp | ELam (Bind Exp) | ...

type Env = [Wh]

data Wh = Type | Nat | Fin Env Exp
  | Pi Env Exp (Bind Exp) | Zero
  | Suc Env Exp | Lam Env (Bind Exp) | Ne Ne

data Ne = ...
\end{verbatim}

In the weak-head normal form type (\texttt{Wh}) above, all
constructors with arguments contain a closed environment of WHNF's,
along with unevaluated expressions delayed by the closure. 
Now let's see how \texttt{infer} works with grammatically enforced
WHNF's.

\begin{verbatim}
infer :: Exp -> TCM Wh
infer (EApp f a) = do
  Pi env _A (Bind _B) <- infer f
  _A' <- infer a
  unless (eval _A env == _A') $
    throwError "Domain not convertible to argument"
  return (eval _B (eval a []):env)
\end{verbatim}

Firstly, the domain \texttt{\_A} must first be evaluated before comparing it
with the argument type \texttt{\_A'}. Secondly, converitbility
(\texttt{(==)}) is now an efficient operation that checks for
syntactic equality before reducing closures. Finally, the returned
type of \texttt{\_B} is evaluated in the extension of the environment
by the evaluation of the argument \texttt{a} in the empty environment.

This environment evaluator approach has the advantage of grammatically
enforcing WHNF in types, unlike expression types, but still can
perform convertibility efficiently. Imagine that we extend our
language with the type of finite sets (\texttt{Fin Env Exp}), which is
indexed by a natural number. If \texttt{(eval \_A env)} and \texttt{\_A'}
are both \texttt{(Fin [2,1024] (Var 0 ** Var 1))}, then we get a
tremendous speedup by comparing them for syntactic equality before evaluating
the closure and then comparing.

\paragraph{WHNF Types without Expressions}

Our proposal is to combine the efficiency benefits of WHNF environment
evaluators, with the software engineering benefits begot by
normal forms defined independentlyn of expressions. In order to do
this, we use the 3 constructor expression grammar from the normal
forms section. Then, we take the \texttt{Wh} grammar from the WHNF
with expressions section, but replace all expressions in the grammar
with a \texttt{Wh}.

\begin{verbatim}
data Exp = EVar Int | ELam (Bind Exp) | EApp Exp Exp

type Env = [Wh]

data Wh = Type | Nat | Fin Env Wh
  | Pi Env Wh (Bind Wh) | Zero
  | Suc Env Wh | Lam Env (Bind Wh) | Ne Ne

data Ne = ...
\end{verbatim}

The \texttt{infer} function is similar to the WHNF with expressions
version, except we rename \texttt{eval} to \texttt{hsub} (which is now a
weak-head hereditary substitution). Additionally, the argument
\texttt{a} is now normalized in the monad, so its variables can be
looked up in an initial environment similar to the one in the normal
forms section.

\begin{verbatim}
infer :: Exp -> TCM Wh
infer (EApp f a) = do
  Pi env _A (Bind _B) <- infer f
  _A' <- infer a
  unless (hsub _A env == _A') $
    throwError "Domain not convertible to argument"
  a'  <- norm a
  return (hsub _B a':env)
\end{verbatim}

Just like in the normal forms section, because our grammar is so
small, functions on our surface language of expressions (like
\texttt{infer} and \texttt{norm}) only need to handle 3 cases.
However, because we do not reduce to full normal form, we still can
implement a reasonable efficient type checker.

\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}


\end{document}

