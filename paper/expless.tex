
\documentclass[preprint,authoryear]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{proof}

\def\turn{\vdash}
\def\conv{\approx}

\newcommand{\fun}[1]{\textmd{#1}}
\newcommand{\cir}[1]{\textcircled{\raisebox{-0.9pt}{#1}}}

\newcommand{\refsec}[1]{Section \ref{sec:#1}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{DRAFT}        % These are ignored unless
%% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Expressionless Weak Head Normal Forms}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Larry Diehl}
           {Portland State University}
           {ldiehl@cs.pdx.edu}

\authorinfo{Tim Sheard}
           {Portland State University}
           {sheard@cs.pdx.edu}

\maketitle

\begin{abstract}
The grammars used for the surface language and core language of a lambda
calculus can enforce correctness properties and guide an efficient
implementation. A common approach to implementing a dependently typed
language with an efficient incremental conversion rule is to use
weak head normal forms (WHNF's), where head constructors contain a closure
composed of an environment of WHNF's and an unevaluated expression. In
our work we explore the idea of removing the dependency of WHNF's on
expressions by making closures contain an environment and a WHNF
(instead of an expression).

Our main contributions are formalizations
of such a programming language architecture for Godel's System T,
namely termination of a weak head environment-based evaluator, and
decidability of an incremental conversion relation. We regard this
formal development as a stepping stone towards towards implementing
similar expressionless WHNF architectures for Dependent Type Theory (DTT).
The independence between expression and WHNF grammars can be exploited
in DTT to make a surface language of expressions only containing
values for function types, and embedding values of all other types in
an initial environment of WHNF's. By having a small grammar of
expressions, dependent type checker implementations only need to
handle a small number of cases. We hope that our formal development
can be expanded in the future to prove the consistency of such a
minimal dependent type checker implementation.
\end{abstract}

\category{D.3.1}{Programming Languages}{Formal Definitions and Theory}

\keywords
normal form, closure, termination

\section{Introduction}

An implementation of a functional programming language can choose
compromises between {\it obvious correctness} and {\it efficiency}. Our motivation is
finding a good collection of compromises for the
implementation of a Dependently Typed Programming Language (DTPL).
One important design choice when implementing a DTPL is what syntactic
grammars of terms should be used for the surface language and any
intermediate languages. The surface language will always be
expressions, as users must be able to write programs containing
redexes.

We can choose to compromise {\it obvious correctness} by making
the type checker take an expression as a type,
and the normalizer return an expression as a type. Expressions
make no grammatical guarantees about having removes redexes, so we
must constantly worry about whether a term has been reduced.
Alternatively, we can choose to compromise {\it efficiency} by making
the type checker take a normal form as a type, and the normalizer
return a normal form as a type. Although a grammar of normal forms can
guarantee the lack of redexes, it comes at the cost of prematurely
fully evaluating terms with expensive compuations.

A popular way to reconcile
{\it obvious correctness} and {\it efficiency} is to make
the type checker and normalizer use a
Weak Head Normal Form (WHNF) grammar. Head constructors of a WHNF
contain a closure, often represented as unevaluated expressions and the
environment of WHNF's captured by the closure. Unfortunately, this
introduces a new compromise of {\it dependence}, namely the dependence
of the WHNF grammar on the expression grammar.

In this work we explore the idea of making closures
{\it expressionless}. That is, a WHNF closure contains an environment,
along with other WHNF's rather than expressions. This allows us to
drastically shrink the size of the type checker and normalizer by
making expressions only contain values for $\Pi$ types, and exposing
values of all other types as WHNF's in an initial environment.

The motivation for our work is to implement an obviously correct,
efficient, and small dependent type checker by exploiting
expressionless weak head normal forms. We will describe this
motivation in detail in \refsec{motivation}. However, we are also
interested in formalizing the consistency of such a dependent type
theory. While such a formalization is ongoing work, our
{\bf main contributions} are formalized for G{\"o}del's System T:

\begin{itemize}
\item A grammar of expressionless WHNF's containing values for {\it all
  types} in the language, a small grammar of expressions only
  containing values for {\it function types}, and an initial environment of
  WHNF's exposing values for {\it non-function types}.
\item A termination proof of normalizing expressions to WHNF, which is
  a corollary of a weak head hereditary substitution proof (this
  normalizer and hereditary substitution would be used in typing rules
  like application in a dependently typed setting.)
\item A decidability proof of a weak head converbility relation that
  incrementally reduces closures only after a syntactic equality check
  fails (this converbility relation would be used in the conversion
  rule in a dependently typed setting.)
\end{itemize}

%% in the style of Big-Step Normalization by Altenkirch and Chapman

\section{Dependent Type Checking}
\label{sec:motivation}

The motivation for weak head normal forms in dependent type theory is
to strike a balance between obvious correctness and efficiency in the
implementation of the type checker. 

\subsection{Specification}

Before considering the {\it implementation} of a dependent type checker,
let's consider the complexities that arise in the
{\it specification} of dependent typing rules.
The rule for dependent application exposes the complexities
that arise in dependent type checking.

$$
\infer[]
  {\Gamma \turn f~a : B[a]}
{
  \Gamma \turn f : \Pi~A~B
  &
  \Gamma \turn a : A
}
$$

There are 3 interesting
constraints in this rule, and all of them have to do with what is
happening in the {\it type} position of the typing judgement.

\begin{enumerate}
\item The type of the function $f$ must be $\Pi~A~B$, thus it must at
  least be in head-normal form.
\item The type of $a$ must be equal to the domain of $f$.
\item The type of the application $f a$ is the substitution of $a$ for
  the bound variable in $B$.
\end{enumerate}

Because redexes can appear in dependent types, it is sometimes
necessary for types to be reduced before a typing rule becomes valid.
Otherwise, the following 3 problems can violate the aforementioned
constraints.

\begin{enumerate}
\item The type of $f$ is a redex like $(\lambda x. x)~(\Pi~A~B)$.
\item The type of $a$ is $A$, while the codomain of $f$ is a redex like
$(\lambda x. x)~A$.
\item Substituting the term $a$ into $B$ can create a redex, resulting
  in problems 1 and 2 somewhere else in type checking.
\end{enumerate}

The specification of typing rules can sweep all of these
problems under the rug by via a conversion rule.

$$
\infer[]
  {\Gamma \turn a : B}
{
  \Gamma \turn a : A
  &
  A \conv B
}
$$

The conversion relation $\conv$ considers types to equal after
reducing redexes and performing capture-avoiding substitutions. Hence $\conv$ is at least $\alpha$-$\beta$
equality, but it may enforce additional forms of equality, such as $\eta$.

\subsection{Implementation}

Dependent type theory can be specified succinctly because the
conversion relation is nondeterministic. In contrast, an
implementation of a dependent type checker in a functional language
must be deterministic. Additionally, the implementation must be
reasonably efficient to execute. 

Below we consider 4 different Haskell implementations of type checking
function application. The 4 implementations differ in the syntactic
grammar used for types: First expressions, then normal forms, then
weak head normal forms with expression closures, and finally our novel
weak head normal forms with expressionless closures.

\paragraph{Expression Types}

Consider the function case of a type checker for expressions below.
For simplicity, we assume that all terms are annotated enough for us
to infer types instead of checking them. Additionally, the terms are
in de Bruijn notation so we do not need to deal with $\alpha$-renaming.
The input of \texttt{infer} is an expression
value, and the output is an expression type in a type checking monad.

\begin{verbatim}
infer :: Exp -> TCM Exp
infer (App f a) = do
  Pi _A _B <- infer f
  _A'      <- infer a
  unless (_A == _A') $
    throwError "Domain not convertible to argument"
  return . norm (_B `sub` a)
\end{verbatim}

Whether or not the code above is correct depends on the rest of the
implementation of \texttt{infer}, what the conversion function
\texttt{(==)} does, and what the normalization function
\texttt{norm} does. Because the grammar of expressions is so flexible,
we can never be sure if a given expression is in normal form,
weak head normal form, or if the head position is a redex.

The easiest way to get the implementation correct is to make
\texttt{norm} compute to full normal form. This way,
conversion \texttt{(==)} can be a simple syntactic equality. The
(albeit more complex) implementation makes \texttt{norm} compute only
to weak head normal form, and conversion \texttt{(==)} first check
syntactic equality, and then compute away redexes if syntactic
equality fails. Whatever normal form we compute to, we need to make
sure that all cases of \texttt{infer} do it so that we can
successfully match against \texttt{Pi \_A \_B}.

Why is it a good idea to prefer weak head normal forms over normal
forms? The normal forms can get quite large! Consider type checking
the application case where both the type of the argument (\texttt{\_A'})
and the domain (\texttt{\_A}) are the finite set type
\texttt{Fin (2 ** 1024)}. Computing both of these types to normal form
and before comparing them is not practical, while comparing the
terms with the redexes still present is reasonable.

\paragraph{Normal Form Types}

As mentioned above, computing to full normal form is not practical for
implementing a dependent type checker. Nevertheless, let us consider
the advantages obtained by having grammatically enforced normal form
types.

Below is an \texttt{infer} function that takes in an expression
and produces a type in normal form. The conversion function
\texttt{(==)} is just syntactic equality, and definition looks like
the expression version, except for the last line. In the expression
version we first syntactially substitute the argument \texttt{a} into
the codomain \texttt{B}, and then normalize the result. However,
syntactic substitution can create a redex, so it is not a well-formed
operation on normal forms. Instead, we first normalize the argument,
and then hereditarily substitute~\cite{TODO} into the codomain, which
computes away any redex created while substituting.

\begin{verbatim}
infer :: Exp -> TCM Nf
infer (App f a) = do
  Pi _A _B <- infer f
  _A'      <- infer a
  unless (_A == _A') $
    throwError "Domain not equal to argument"
  a' <- norm a
  return (_B `hsub` a')
\end{verbatim}

There are three major benefits we get by having grammatically enforced
normal forms types. The first two advantages have to do with
simplicity in the implementation, and allows us to have a type checker
with a minimal number of cases.

\begin{enumerate}
\item Because the type signature of \texttt{infer} enforces that it
  returns a normal form, our pattern match against \texttt{Pi \_A \_B}
  always works (we can't forget to compute to normal form in some case
  of \texttt{infer}).
\item The conversion function of \texttt{(==)} can be a simple
  equality check, which can even be derived by Haskell.
\item The grammar of expressions and normal forms are defined
  independently, which can be exploited to implement a very small type
  checker!
\end{enumerate}

The last benefit of normal form types, which we call the
 {\it independence} property, needs more explaining. Think of
the expressions as a minimal surface language, and the
normal forms as a core language. The only expression constructors that we
need are variables, functions, and applications. Normal forms have the
same constructors, but also constructors for the types, introduction,
and elimination rules of all non-function types in the languages (e.g.
booleans, natural numbers, lists, trees, products, sums, etc).

\begin{verbatim}
data Exp = EVar Int | ELam (Bind Exp) | EApp Exp Exp

data Nf = Type | Nat | Pi Nf (Bind Nf)
  | Zero | Suc Nf | Lam (Bind Nf) | Ne Ne

data Ne = Var Int | App Ne Nf
  | ElimNat (Bind Nf) Nf (Bind2 Nf) Ne
\end{verbatim}

How is it possible to have such a small expression grammar? Type
formers, constructors, and eliminators for all other types can be
exposed as definitions in an initial environment. If a normal form
primitive has multiple arguments, it can be wrapped with lambdas. This
means that a primitive like \texttt{Suc} is exposed as a variable
whose type is \texttt{Nat => Nat}, so users can even partially apply
them instead of needing to use the fully-applied primitives. 

\begin{verbatim}
zeroT = Nat
zero = Zero

sucT = Nat :=> Nat
suc = Lam (Bind (Suc (Var 0)))

elimNatT = Pi (Nat :=> Type)
  (Bind (Nat
     :=> Pi Nat (Bind (App (Var 1) (Var 0)
            :=> App (Var 1) (Suc (Var 0))))
     :=> Pi Nat (Bind (App (Var 1) (Var 0)))
  ))
elimNat = Lam (Bind (Lam (Bind (Lam (Bind
  ElimNat
    (Bind (Var 1 `App` Var 0))
    (Var 1)
    (Bind2 (Var 4 `App` Var 0 `App` Var 1))
    (Var 3)
  )))))

initCtx = [zeroT, sucT, elimNatT]
initEnv = [zero, suc, elimNat]
\end{verbatim}

The initial context \texttt{initCtx} contains types for all
non-$\Pi$ primitive constructions, and is used by \texttt{infer} in the
variable case.
The initial environment \texttt{initEnv} contains values for all
non-$\Pi$ primitive constructions, and is used by \texttt{norm} in the
variable case.

So what have we achieved by using this technique of embedding primitives
in an initial context and environment? Now our type checker
(\texttt{infer}) and our normalizer (\texttt{norm}) only need to
handle 3 cases, one for each constructor in the expression grammar!
This is quite nice, as a dependent type checker gets complicated
quickly as you add new feature. For example, if we added
Matita-style~\cite{TODO} elaboration of implicit arguments to
metavariables and unification problems, we would only need to
implement it for 3 cases of our \texttt{Exp} grammar.
Furthermore, changes to our primitives no longer require changes to
the type checker.

\paragraph{WHNF Types with Expressions}

So far we have seen a dependent type checker that uses
expression types, which may be done efficiently or inefficiently but
has no grammatical correctness properties, and a checker that uses
normal form types, which is inefficient but has grammatically enforced
correctness properties.

Recall that in the land of expression types we wanted to
make type checking reasonably efficient by
comparing types for syntactic equality before reducing a redex.
A popular way to do this {\it with} grammatically enforced correctness
properties is to implement an environment machine evaluator. Therein
an expression evalutes to a WHNF value, where constructor heads
(like \texttt{Suc}) store an environment along with an unevaluated
expression. The environment only contains WHNF's for all free
variables in the expression, but not for binding positions (like the
\texttt{Lam} body or the \texttt{Pi} codomain).

We are free to choose which terms we would like to contain closures.
For example, maybe we only want closures at binding positions. 

\begin{verbatim}
data Exp = EType | ENat | EPi Exp (Bind Exp)
  | EZero | ESuc Exp | ELam (Bind Exp) | ...

type Env = [Wh]

data Wh = Type | Nat | Fin Env Exp
  | Pi Env Exp (Bind Exp) | Zero
  | Suc Env Exp | Lam Env (Bind Exp) | Ne Ne

data Ne = ...
\end{verbatim}

In the weak head normal form type (\texttt{Wh}) above, all
constructors with arguments contain a closed environment of WHNF's,
along with unevaluated expressions delayed by the closure. 
Now let's see how \texttt{infer} works with grammatically enforced
WHNF's.

\begin{verbatim}
infer :: Exp -> TCM Wh
infer (EApp f a) = do
  Pi env _A (Bind _B) <- infer f
  _A' <- infer a
  unless (eval _A env == _A') $
    throwError "Domain not convertible to argument"
  return (eval _B (eval a []):env)
\end{verbatim}

Firstly, the domain \texttt{\_A} must first be evaluated before comparing it
with the argument type \texttt{\_A'}. Secondly, converitbility
(\texttt{(==)}) is now an efficient operation that checks for
syntactic equality before reducing closures. Finally, the returned
type of \texttt{\_B} is evaluated in the extension of the environment
by the evaluation of the argument \texttt{a} in the empty environment.

This environment evaluator approach has the advantage of grammatically
enforcing WHNF in types, unlike expression types, but still can
perform convertibility efficiently. Imagine that we extend our
language with the type of finite sets (\texttt{Fin Env Exp}), which is
indexed by a natural number. If \texttt{(eval \_A env)} and \texttt{\_A'}
are both \texttt{(Fin [2,1024] (Var 0 ** Var 1))}, then we get a
tremendous speedup by comparing them for syntactic equality before evaluating
the closure and then comparing.

\paragraph{WHNF Types without Expressions}

Our proposal is to combine the efficiency benefits of WHNF environment
evaluators, with the software engineering benefits begot by
normal forms defined independentlyn of expressions. In order to do
this, we use the 3 constructor expression grammar from the normal
forms section. Then, we take the \texttt{Wh} grammar from the WHNF
with expressions section, but replace all expressions in the grammar
with a \texttt{Wh}.

\begin{verbatim}
data Exp = EVar Int | ELam (Bind Exp) | EApp Exp Exp

type Env = [Wh]

data Wh = Type | Nat | Fin Env Wh
  | Pi Env Wh (Bind Wh) | Zero
  | Suc Env Wh | Lam Env (Bind Wh) | Ne Ne

data Ne = ...
\end{verbatim}

The \texttt{infer} function is similar to the WHNF with expressions
version, except we rename \texttt{eval} to \texttt{hsub} (which is now a
weak head hereditary substitution). Additionally, the argument
\texttt{a} is now normalized in the monad, so its variables can be
looked up in an initial environment similar to the one in the normal
forms section.

\begin{verbatim}
infer :: Exp -> TCM Wh
infer (EApp f a) = do
  Pi env _A (Bind _B) <- infer f
  _A' <- infer a
  unless (hsub _A env == _A') $
    throwError "Domain not convertible to argument"
  a'  <- norm a
  return (hsub _B a':env)
\end{verbatim}

Just like in the normal forms section, because our grammar is so
small, functions on our surface language of expressions (like
\texttt{infer} and \texttt{norm}) only need to handle 3 cases.
However, because we do not reduce to full normal form, we still can
implement a reasonable efficient type checker.

Finally, in the previous sections we assume that all terms are
inferable to simplify the presentation. In reality not all of our expressions
are inferable. To remedy the situation, we could switch the
expressions to a bidirectional~\cite{TODO} syntax, and have both a \texttt{check}
function in addition to \texttt{infer}. Note that doing this is much
easier when our expression grammar only has terms for $\Pi$ types, as
opposed to terms for all types in the WHNF core language!

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}

\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.


\end{document}

